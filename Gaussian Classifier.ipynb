{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"font-family:opensans; font-size:1.5em;\">Gaussian Classifiers for Digits and Spam</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "from numpy import linalg as LA\n",
    "from scipy.io import loadmat\n",
    "\n",
    "##data cleaning\n",
    "spam = loadmat('spam_data.mat')\n",
    "mnist = loadmat('mnist_data.mat')\n",
    "\n",
    "def l2_norm(vector):\n",
    "    if np.linalg.norm(vector) == 0:\n",
    "        return vector.astype(np.double)\n",
    "    return vector.astype(np.double)/np.linalg.norm(vector)\n",
    "def normalize(inpt):\n",
    "    for i, item in enumerate(inpt):\n",
    "        inpt[i] = l2_norm(item)\n",
    "\n",
    "#make sure type double\n",
    "mnist_training = mnist['training_data'].astype(np.double)\n",
    "mnist_test = mnist['test_data'].astype(np.double)\n",
    "spam_training = spam['training_data'].astype(np.double)\n",
    "spam_test = spam['test_data'].astype(np.double)\n",
    "\n",
    "#normalize all\n",
    "normalize(mnist_training)\n",
    "normalize(mnist_test)\n",
    "normalize(spam_training)\n",
    "normalize(spam_test)\n",
    "\n",
    "#flattening\n",
    "mnist_labels = [l[0] for l in mnist['training_labels']]\n",
    "spam_labels = [l[0] for l in spam['training_labels']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting mean and Covariances per label\n",
    "mnist_map = dict((k, []) for k in range(10))\n",
    "for i in range(len(mnist_labels)):\n",
    "    mnist_map[mnist_labels[i]].append(mnist_training[i])\n",
    "\n",
    "mnist_mean = {}\n",
    "mnist_cov = {}\n",
    "\n",
    "for label in mnist_map:\n",
    "    matrix = np.asmatrix(mnist_map[label])\n",
    "    m = matrix.mean(axis = 0)\n",
    "    mnist_mean[label] = m\n",
    "    cov = np.cov(matrix.T)\n",
    "    mnist_cov[label] = cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def random_sample(s, train_data, labels, n):\n",
    "    labels = np.array(labels)\n",
    "    random.seed(s)\n",
    "    shuffled = random.sample(range(len(train_data)), len(train_data)) \n",
    "    validation_index = shuffled[:n]\n",
    "    training_index = shuffled[n:]\n",
    "\n",
    "    return train_data[validation_index], labels[validation_index], train_data[training_index], labels[training_index]\n",
    "\n",
    "mnist_validation_set, mnist_validation_labels, mnist_training_set, mnist_training_labels = random_sample(120, mnist_training, mnist_labels, 10000)\n",
    "\n",
    "spam_validation_set, spam_validation_labels, spam_training_set, spam_training_labels = random_sample(120, spam_training, spam_labels, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_estimate(training_dat, training_labels, spam = False):\n",
    "    #for data, divide by label\n",
    "    if spam:\n",
    "        n = 2\n",
    "    else:\n",
    "        n = 10\n",
    "    dim = training_dat[0].shape[0]\n",
    "\n",
    "    _map = dict((k, []) for k in range(n))\n",
    "    for i in range(len(training_labels)):\n",
    "        _map[training_labels[i]].append(training_dat[i])\n",
    "\n",
    "    training_mean = {}\n",
    "    training_cov = {}\n",
    "    n = len(training_dat)\n",
    "    priors = []\n",
    "    \n",
    "    #dictionary of means and covariances\n",
    "    for label in _map:\n",
    "        matrix = np.asmatrix(_map[label])\n",
    "        m = matrix.mean(axis = 0)\n",
    "        training_mean[label] = m\n",
    "        cov = np.cov(matrix.T)\n",
    "        training_cov[label] = cov\n",
    "        priors.append(len(_map[label])/n)\n",
    "        \n",
    "    #Averaged coveriance and inverse, logdet\n",
    "    training_cov = np.array(list(training_cov.values())).mean(axis=0)\n",
    "    training_cov += np.eye(dim, dtype=float)*0.0000001\n",
    "    #Inverse of covariance\n",
    "    I = np.identity(dim)\n",
    "    inv_Cov = LA.solve(training_cov, I)\n",
    "    sign,logdet = LA.slogdet(covariance)\n",
    "    logdet *= sign\n",
    "    \n",
    "    return training_mean, inv_Cov, priors, logdet\n",
    "\n",
    "def error_rt(predicted, labels):\n",
    "    return (1 - np.mean([predicted[i] == labels[i] for i in range(len(labels))]))\n",
    "\n",
    "def Qc(x, mu, pi, inv_Cov, logdet):\n",
    "    x = np.asmatrix(x)\n",
    "    return float(-0.5*logdet + np.log(pi) - (0.5*(x - mu)*inv_Cov*(x - mu).T))\n",
    "\n",
    "def LDA_predict(x, means, priors, inv_Cov, logdet):\n",
    "    #posteriors\n",
    "    posterior_label = {}\n",
    "    for label, mean in means.items():\n",
    "        Q_val = Qc(x, mean, priors[label], inv_Cov, logdet)\n",
    "        posterior_label[Q_val] = label\n",
    "    maxi = max(posterior_label)\n",
    "    return posterior_label[maxi]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sets = [100, 200, 500, 1000, 2000, 5000, 10000, 30000, 50000]\n",
    "t_errors = []\n",
    "v_errors = []\n",
    "\n",
    "for n in training_sets:\n",
    "    dat = mnist_training_set[:n]\n",
    "    lab = mnist_training_labels[:n]\n",
    "    \n",
    "    predicted_train = []\n",
    "    predicted_valid = []\n",
    "    mean, inv_cov, priors, logdet = mle_estimate(dat, lab)\n",
    "    \n",
    "    for x in dat:\n",
    "        predicted_train.append(LDA_predict(x, mean, priors, inv_cov, logdet))\n",
    "    for x in mnist_validation_set:\n",
    "        predicted_valid.append(LDA_predict(x, mean, priors, inv_cov, logdet))\n",
    "    \n",
    "    t_errors.append(error_rt(predicted_train,lab))\n",
    "    v_errors.append(error_rt(predicted_valid, mnist_validation_labels))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving most recent variables \n",
    "final_LDA_mean_MNIST, final_LDA_inv_cov_MNIST = mean, inv_cov\n",
    "final_LDA_logdet_MNIST, final_LDA_priors_MNIST = logdet, priors\n",
    "final_LDA_t_errors, final_LDA_v_errors = t_errors, v_errors\n",
    "final_predicted_train,final_lab = predicted_train, lab\n",
    "final_predicted_valid = predicted_valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** LDA Error Rate Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##For calculating errors classified by digit class\n",
    "def classified_error(p, l, n):\n",
    "    correct = np.repeat(0, n)\n",
    "    incorrect = np.repeat(0, n)\n",
    "    for i in range(len(l)):\n",
    "        if p[i] == l[i]:\n",
    "            correct[l[i]]+=1\n",
    "        else:\n",
    "            incorrect[l[i]]+=1\n",
    "    tot = correct+incorrect\n",
    "    return 1- correct/tot\n",
    "\n",
    "dig_tr_error = classified_error(final_predicted_train, final_lab, 10)\n",
    "dig_v_error = classified_error(final_predicted_valid, mnist_validation_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.arange(10)\n",
    "bar_width = 0.35\n",
    "\n",
    "fig, axs= plt.subplots(1, 2, figsize=(12,5))\n",
    "a, b = axs[0],axs[1]\n",
    "a.plot(training_sets, final_LDA_t_errors, label = \"Training Error\")\n",
    "a.plot(training_sets, final_LDA_v_errors, label = \"Validation Error\")\n",
    "a.legend(loc='upper right')\n",
    "a.set_title(\"MNIST LDA Error\")\n",
    "a.set_xlabel(\"# of Training Points\")\n",
    "a.set_ylabel(\"Error Rate\")\n",
    "\n",
    "b.bar(index, dig_tr_error, bar_width, alpha=0.4, color='black', label='Training Error')\n",
    "b.bar(index + bar_width, dig_v_error, bar_width,alpha=0.4, color='rosybrown', label='Validation Error')\n",
    "b.set_title(\"MNIST LDA Error by Digit\")\n",
    "b.set_xlabel(\"Digit Class\")\n",
    "b.set_ylabel(\"Error Rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_estimate(training_dat, training_labels, spam = False):\n",
    "    if spam:\n",
    "        n = 2\n",
    "    else:\n",
    "        n = 10\n",
    "        \n",
    "    mnist_map = dict((k, []) for k in range(n))\n",
    "    dim = training_dat[0].shape[0]\n",
    "        \n",
    "    for i in range(len(training_labels)):\n",
    "            mnist_map[training_labels[i]].append(training_dat[i])\n",
    "\n",
    "    training_mean = dict((k, []) for k in range(n))\n",
    "    training_inverse_cov = dict((k, []) for k in range(n))\n",
    "    training_logdet = dict((k, []) for k in range(n))\n",
    "    priors = []\n",
    "    n = len(training_dat)\n",
    "    \n",
    "    #dictionary of means and covariances\n",
    "    for label in mnist_map:\n",
    "        matrix = np.asmatrix(mnist_map[label])\n",
    "        m = matrix.mean(axis = 0)\n",
    "        training_mean[label] = m\n",
    "        \n",
    "        cov = np.cov(matrix.T) + np.eye(dim, dtype=float)*0.0000001\n",
    "        I = np.identity(dim)\n",
    "        inv_Cov = LA.solve(cov, I)\n",
    "        training_inverse_cov[label] = inv_Cov\n",
    "        \n",
    "        sign,logdet = LA.slogdet(cov)\n",
    "        logdet *= sign\n",
    "        training_logdet[label] = logdet\n",
    "        priors.append(len(mnist_map[label])/n)\n",
    "    \n",
    "    return training_mean, priors, training_inverse_cov, training_logdet\n",
    "\n",
    "def error_rt(predicted, labels):\n",
    "    return (1 - np.mean([predicted[i] == labels[i] for i in range(len(labels))]))\n",
    "\n",
    "def Qc(x, mu, pi, inv_Cov, logdet):\n",
    "    x = np.asmatrix(x)\n",
    "    return float(-0.5*logdet + np.log(pi) - (0.5*(x - mu)*inv_Cov*(x - mu).T))\n",
    "\n",
    "def QDA_predict(x, means, priors, inv_Covs, logdets):\n",
    "    posterior_label = {}\n",
    "    for label, mean in means.items():\n",
    "        Q_val = Qc(x, mean, priors[label], inv_Covs[label], logdets[label])\n",
    "        posterior_label[Q_val] = label\n",
    "    maxi = max(posterior_label)\n",
    "    return posterior_label[maxi]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sets = [100, 200, 500, 1000, 2000, 5000, 10000, 30000, 50000]\n",
    "t_errors = []\n",
    "v_errors = []\n",
    "\n",
    "for n in training_sets:\n",
    "    dat = mnist_training_set[:n]\n",
    "    lab = mnist_training_labels[:n]\n",
    "    \n",
    "    predicted_train = []\n",
    "    predicted_valid = []\n",
    "    training_mean, priors, training_inverse_cov, training_logdet = mle_estimate(dat, lab)\n",
    "\n",
    "    for x in dat:\n",
    "        predicted_train.append(QDA_predict(x, training_mean, priors, training_inverse_cov, training_logdet))\n",
    "    for x in mnist_validation_set:\n",
    "        predicted_valid.append(QDA_predict(x, training_mean, priors, training_inverse_cov, training_logdet))\n",
    "    \n",
    "    t_errors.append(error_rt(predicted_train,lab))\n",
    "    v_errors.append(error_rt(predicted_valid, mnist_validation_labels))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_QDA_mean_MNIST, final_QDA_inv_cov_MNIST = training_mean, training_inverse_cov\n",
    "final_QDA_logdet_MNIST, final_QDA_priors_MNIST = training_logdet, priors\n",
    "final_QDA_t_errors, final_QDA_v_errors = t_errors, v_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dig_tr_error_QDA = classified_error(predicted_train, final_lab, 10)\n",
    "dig_v_error_QDA = classified_error(predicted_valid, mnist_validation_labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QDA Error Rate Plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.arange(10)\n",
    "bar_width = 0.35\n",
    "\n",
    "index = np.arange(10)\n",
    "bar_width = 0.35\n",
    "\n",
    "fig, axs= plt.subplots(1, 2, figsize=(12,5))\n",
    "a, b = axs[0],axs[1]\n",
    "a.plot(training_sets, final_LDA_t_errors, label = \"Training Error\")\n",
    "a.plot(training_sets, final_LDA_v_errors, label = \"Validation Error\")\n",
    "a.legend(loc='upper right')\n",
    "a.set_title(\"MNIST LDA Error\")\n",
    "a.set_xlabel(\"# of Training Points\")\n",
    "a.set_ylabel(\"Error Rate\")\n",
    "\n",
    "b.bar(index, dig_tr_error, bar_width, alpha=0.4, color='black', label='Training Error')\n",
    "b.bar(index + bar_width, dig_v_error, bar_width,alpha=0.4, color='rosybrown', label='Validation Error')\n",
    "b.set_title(\"MNIST LDA Error by Digit\")\n",
    "b.set_xlabel(\"Digit Class\")\n",
    "b.set_ylabel(\"Error Rate\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axs= plt.subplots(1, 2, figsize=(12,5))\n",
    "a, b = axs[0],axs[1]\n",
    "a.plot(training_sets, final_QDA_t_errors, label = \"Training Error\")\n",
    "a.plot(training_sets, final_QDA_v_errors, label = \"Validation Error\")\n",
    "a.legend(loc='upper right')\n",
    "a.set_title(\"MNIST QDA Error\")\n",
    "a.set_xlabel(\"# of Training Points\")\n",
    "a.set_ylabel(\"Error Rate\")\n",
    "\n",
    "b.bar(index, dig_tr_error_QDA, bar_width, alpha=0.4, color='black', label='Training Error')\n",
    "b.bar(index + bar_width, dig_v_error_QDA, bar_width,alpha=0.4, color='rosybrown', label='Validation Error')\n",
    "b.set_title(\"MNIST QDA Error by Digit\")\n",
    "b.set_xlabel(\"Digit Class\")\n",
    "b.set_ylabel(\"Error Rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"page-break-before: always\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_csv(y_test):\n",
    "    y_test = y_test.astype(int)\n",
    "    df = pd.DataFrame({'Category': y_test})\n",
    "    df.index += 1  # Ensures that the index starts at 1. \n",
    "    df.to_csv('submission.csv', index_label='Id')\n",
    "results_to_csv(np.array(predicted_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KAGGLE SPAM DO NOT RE-RUN\n",
    "mean, inv_cov, priors, logdet = mle_estimate(spam_training_set, spam_training_labels, True)\n",
    "predicted_test = []\n",
    "for x in spam_test:\n",
    "    predicted_test.append(LDA_predict(x, mean, priors, inv_cov, logdet))\n",
    "\n",
    "t_errors.append(error_rt(predicted_train,lab))\n",
    "\n",
    "##KAGGLE MNIST DO NOT RE-RUN\n",
    "predicted_test = []\n",
    "for x in mnist_test:\n",
    "    predicted_test.append(LDA_predict(x, final_mean, final_priors, final_inv_cov, final_logdet))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
