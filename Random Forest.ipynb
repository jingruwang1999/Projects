{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\" font-size:1.3em;\">**Decision Trees for Classification**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "spam_data = loadmat(\"spam_data.mat\")\n",
    "spam_train = spam_data['training_data']\n",
    "spam_labels = spam_data['training_labels'][0]\n",
    "spam_test = spam_data['test_data']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\" font-size:1.3em;\">**Implement Decision Trees**</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This is the starter code and some suggested architecture we provide you with. \n",
    "But feel free to do any modifications as you wish or just completely ignore \n",
    "all of them and have your own implementations.\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, data, y, indices, split_rule, label = None):\n",
    "        \"\"\"\n",
    "        TODO: initialization of a decision tree\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.y = y\n",
    "        self.indices = indices\n",
    "        self.split_rule = split_rule\n",
    "        self.label = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def entropy(y):\n",
    "        \"\"\"\n",
    "        TODO: implement a method that calculates the entropy given all the labels\n",
    "        \"\"\"\n",
    "        proportions = np.unique(y, return_counts = True)[1]/len(y)\n",
    "        proportions = proportions[proportions!=0]\n",
    "\n",
    "        return -sum(proportions*np.log2(proportions))\n",
    "    @staticmethod\n",
    "    def binary_entropy(num_c, total):\n",
    "        if total == 0:\n",
    "            return total\n",
    "        p = num_c/total\n",
    "        left = -p*np.log2(p) if p != 0 else 0\n",
    "        right = -(1-p)*np.log2(1-p) if (1-p) != 0 else 0\n",
    "        return left + right\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def H_after(left_c, left_length, right_c, right_length):\n",
    "        n = left_length + right_length\n",
    "        \n",
    "        HSl = DecisionTree.binary_entropy(left_c, left_length)\n",
    "        HSr = DecisionTree.binary_entropy(right_c, right_length)\n",
    "        return (left_length*HSl + right_length*HSr)/n\n",
    "    @staticmethod\n",
    "    def map_sort(X, y):\n",
    "        X, y = list(X), list(y)\n",
    "        zipped = np.array(sorted(zip(X, y)))\n",
    "        return zipped[:,0], zipped[:,1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def segmenter(X, y, random_forest = False):\n",
    "        \"\"\"\n",
    "        TODO: compute entropy gain for all single-dimension splits,\n",
    "        return the feature and the threshold for the split that\n",
    "        has maximum gain\n",
    "        \"\"\"\n",
    "        n = len(y)\n",
    "        max_gain = -1\n",
    "        best_feature = 0\n",
    "        best_threshold = 0\n",
    "        f = len(X[0])\n",
    "        H = DecisionTree.entropy(y)\n",
    "        \n",
    "        if random_forest:\n",
    "            features = random.sample(range(f), int(np.sqrt(f)))\n",
    "            X = np.array(random.choices(X, k = len(X)))\n",
    "        else:\n",
    "            features = range(f)\n",
    "            \n",
    "        for feat in features:\n",
    "            entropies = []\n",
    "            column = X[:, feat]\n",
    "            \n",
    "            left_length, right_length = 0, len(column)\n",
    "            left_c, right_c = 0, sum(y == y[0])\n",
    "            \n",
    "            sorted_X, sorted_y = DecisionTree.map_sort(column, y)\n",
    "\n",
    "            c = y[0]\n",
    "            for i in range(n):\n",
    "                left_length += 1\n",
    "                right_length -= 1\n",
    "                \n",
    "                if sorted_y[i] == c:\n",
    "                    left_c += 1\n",
    "                    right_c -= 1\n",
    "                    \n",
    "                if (i < n - 1) and (sorted_X[i] == sorted_X[i+1]):\n",
    "                    entropies.append(100)\n",
    "                else:\n",
    "                    entropies.append(DecisionTree.H_after(left_c, left_length, right_c, right_length))\n",
    "                    \n",
    "            j = np.argmin(entropies)\n",
    "            info_gain = H - entropies[j]\n",
    "\n",
    "            if info_gain >= max_gain:\n",
    "                best_feature = feat\n",
    "                best_threshold = sorted_X[j]\n",
    "                max_gain = info_gain\n",
    "\n",
    "                    \n",
    "        return (best_feature, best_threshold)\n",
    "                \n",
    "    def split(self, feature, thresh):\n",
    "        \"\"\"\n",
    "        TODO: implement a method that return a split of the dataset given an index of the feature and\n",
    "        a threshold for it\n",
    "        \"\"\"\n",
    "        if self.label:\n",
    "            return\n",
    "        \n",
    "        indices = self.indices\n",
    "        left, right = [], []\n",
    "        c = self.y[0]\n",
    "        left_c = 0\n",
    "        right_c = 0\n",
    "        \n",
    "        for i in indices:\n",
    "            if self.data[i, feature] > thresh:\n",
    "                left.append(i)\n",
    "                if self.data[i, feature] == c:\n",
    "                    left_c+=1\n",
    "            else:\n",
    "                right.append(i)\n",
    "                if self.data[i, feature] == c:\n",
    "                    right_c+=1\n",
    "\n",
    "        labels_left, labels_right = self.y[left], self.y[right]\n",
    "        label_l, label_r = None, None\n",
    "        \n",
    "        H = DecisionTree.entropy(self.y)\n",
    "        H_after = DecisionTree.H_after(left_c, len(labels_left), right_c, len(labels_right))\n",
    "        info_gain = H - H_after\n",
    "        \n",
    "        if info_gain <= 1e-20 and len(labels_left) > 0:\n",
    "            label_l = round(np.mean(labels_left))\n",
    "        if info_gain <= 1e-20 and len(labels_right) > 0:\n",
    "            label_r = round(np.mean(labels_right))\n",
    "\n",
    "        len_left, len_right = len(left), len(right)\n",
    "        \n",
    "        if len_left != 0:\n",
    "            self.left = DecisionTree(self.data, self.y, left, None, label_l)\n",
    "        if len_right != 0:\n",
    "            self.right = DecisionTree(self.data, self.y, right, None, label_r)\n",
    "        return\n",
    "    \n",
    "    def fit(self, depth = 0, d = False, random_forest = False):\n",
    "        \"\"\"\n",
    "        TODO: fit the model to a training set. Think about what would be \n",
    "        your stopping criteria\n",
    "        \"\"\"\n",
    "        if (self.label != None) or (self == None):\n",
    "            return\n",
    "        \n",
    "        X = self.data[self.indices]\n",
    "        y = self.y[self.indices]\n",
    "        \n",
    "        if d:\n",
    "            if (depth == 0):\n",
    "                if len(np.unique(y)) == 1:\n",
    "                    self.label = self.y[0]\n",
    "                else:\n",
    "                    self.label = round(np.mean(y))\n",
    "                return\n",
    "            else:\n",
    "                feature, thresh = DecisionTree.segmenter(X, y, random_forest)\n",
    "                self.split_rule = (feature, thresh)\n",
    "                self.split(feature, thresh)\n",
    "                if self.left:\n",
    "                    self.left.fit(depth - 1, True)\n",
    "                if self.right:\n",
    "                    self.right.fit(depth - 1, True)\n",
    "        else:\n",
    "            feature, thresh = DecisionTree.segmenter(X, y, random_forest)\n",
    "            self.split(feature, thresh)\n",
    "            \n",
    "            self.left.fit()\n",
    "            self.right.fit()\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        TODO: predict the labels for input data \n",
    "        \"\"\"\n",
    "                \n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            ##go through the tree\n",
    "            temp = self\n",
    "            \n",
    "            while temp:\n",
    "                if temp.label != None:\n",
    "                    predictions.append(temp.label)\n",
    "                    break\n",
    "                        \n",
    "                feature, threshold = temp.split_rule\n",
    "                if x[feature] > threshold:\n",
    "                    if temp.left == None:\n",
    "                        predictions.append(round(np.mean(temp.y[temp.indices])))\n",
    "                        break\n",
    "                    else:\n",
    "                        temp = temp.left\n",
    "                else:\n",
    "                    if temp.right == None:\n",
    "                        predictions.append(round(np.mean(temp.y[temp.indices])))\n",
    "                        break\n",
    "                    else:\n",
    "                        temp = temp.right\n",
    "                        \n",
    "        return predictions\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        TODO: one way to visualize the decision tree is to write out a __repr__ method\n",
    "        that returns the string representation of a tree. Think about how to visualize \n",
    "        a tree structure. You might have seen this before in CS61A.\n",
    "        \"\"\"\n",
    "        string = \"(\" + str(self.split_rule) + \")\\n\"\n",
    "        \n",
    "        if self.left:\n",
    "            string += \"left:\" + repr(self.left)\n",
    "        else:\n",
    "            string += \"left No Node\\n\"\n",
    "        if self.right:\n",
    "            string += \"right:\" + repr(self.right)\n",
    "        else:\n",
    "            string += \"right No Node\\n\"\n",
    "        return string\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(lst):\n",
    "    return lst.astype(np.double)/np.linalg.norm(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\" font-size:1.3em;\">**Random Forests**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_predict(predict_data, num_trees, depth, train_data, labels):\n",
    "    predicted = np.array([0.0]*len(predict_data)).astype('float')\n",
    "    for i in range(num_trees):\n",
    "        tree = DecisionTree(train_data, labels, range(len(labels)), None)\n",
    "        tree.fit(depth, True, True)\n",
    "        predicted += np.array(tree.predict(predict_data)).astype('float')\n",
    "    return np.round(predicted/num_trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\" font-size:1.3em;\">**Implementation Details**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Categorical features were converted to numbers and missing values  \n",
    "were replaced with most frequently appearing values using sklearn's  \n",
    "Implementer\n",
    "2. Stopping criterion was the depth. Leaves are created when  \n",
    "information gain is less than or equal to 1-e20  \n",
    "3. Random Forests were implemented with the decision tree,  \n",
    "but randomly selected root(M) features and then the forest with  \n",
    "n number of trees' predictions are averaged out to give majority prediction.  \n",
    "4. I sped up the training using the trick taught in lecture using the  \n",
    "sorted labels to determine thresholds that minimizes entropy to maximize  \n",
    "info gain.  \n",
    "5. I normalized vectors and that improved accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\" font-size:1.3em;\">**Performance Evaluation**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split\n",
    "def train_valid_split(data, labels, seed = 12, percent_train = 0.8):\n",
    "    n = len(data)\n",
    "    train_count = int(percent_train*n)\n",
    "    random.seed(seed)\n",
    "    random_indices = random.sample(range(n), len(labels))\n",
    "    train_indices = random_indices[:train_count]\n",
    "    validation_indices = random_indices[train_count:]\n",
    "\n",
    "    training_data = data[train_indices]\n",
    "    training_labels = labels[train_indices]\n",
    "\n",
    "    validation_data = data[validation_indices]\n",
    "    validation_labels = labels[validation_indices]\n",
    "    \n",
    "    return training_data, training_labels, validation_data, validation_labels\n",
    "\n",
    "training_data_spam, training_labels_spam, validation_data_spam, validation_labels_spam = train_valid_split(spam_train, spam_labels)\n",
    "training_data_spam = l2_normalize(training_data_spam)\n",
    "validation_data_spam = l2_normalize(validation_data_spam)\n",
    "spam_test = l2_normalize(spam_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "#Data cleaning for Titanic\n",
    "\n",
    "titanic_training = pd.read_csv(\"titanic_training.csv\")\n",
    "titanic_test = pd.read_csv(\"titanic_testing_data.csv\")\n",
    "\n",
    "\n",
    "titanic_training['sex'] = [1 if val==\"female\" else 0 for val in titanic_training['sex']]\n",
    "titanic_test['sex'] = [1 if val==\"female\" else 0 for val in titanic_test['sex']]\n",
    "\n",
    "titanic_training_labels = titanic_training['survived'].copy()\n",
    "titanic_training_labels[np.isnan(titanic_training_labels)] = 1\n",
    "titanic_training = titanic_training.drop(columns=[\"ticket\", \"cabin\", \"survived\"])\n",
    "titanic_test = titanic_test.drop(columns = ['cabin', 'ticket'])\n",
    "\n",
    "replace = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "replace.fit(titanic_training)\n",
    "\n",
    "titanic_training = pd.DataFrame(replace.transform(titanic_training), index = titanic_training.index, columns = titanic_training.columns)\n",
    "titanic_test = pd.DataFrame(replace.transform(titanic_test), index = titanic_test.index, columns = titanic_test.columns)\n",
    "\n",
    "titanic_training = titanic_training.join(pd.get_dummies(titanic_training['embarked'])).drop(columns = ['embarked'])\n",
    "titanic_test = titanic_test.join(pd.get_dummies(titanic_test['embarked'])).drop(columns = ['embarked'])\n",
    "titanic_training = titanic_training.values.astype(float)\n",
    "titanic_labels = titanic_training_labels.values.astype(float)\n",
    "titanic_test = titanic_test.values.astype(float)\n",
    "\n",
    "training_data_titanic, training_labels_titanic, validation_data_titanic, validation_labels_titanic = train_valid_split(titanic_training, titanic_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training for spam Decision Tree:  0.7918781725888325\n",
      "validation for spam Decision Tree:  0.8067632850241546\n",
      "training for spam Forest:  0.7918781725888325\n",
      "validation for spam Forest:  0.8067632850241546\n",
      "training for titanic Decision Tree:  0.77125\n",
      "validation for titanic Decision Tree:  0.705\n",
      "training for titanic Forest:  0.77\n",
      "validation for titanic Forest:  0.7\n"
     ]
    }
   ],
   "source": [
    "##Decision Tree Accuracies (Takes a while to run)\n",
    "decision_tree = DecisionTree(training_data_spam, training_labels_spam, range(len(training_labels_spam)), None)\n",
    "decision_tree.fit(3, True)\n",
    "training_spam = np.mean(decision_tree.predict(training_data_spam) == training_labels_spam)\n",
    "validation_spam = np.mean(decision_tree.predict(validation_data_spam) == validation_labels_spam)\n",
    "\n",
    "forest_training_spam = random_forest_predict(training_data_spam, 20, 4, training_data_spam, training_labels_spam)\n",
    "forest_validation_spam = random_forest_predict(validation_data_spam, 20, 4, training_data_spam, training_labels_spam)\n",
    "training_spam_forest = np.mean(forest_training_spam == training_labels_spam)\n",
    "validation_spam_forest = np.mean(forest_validation_spam == validation_labels_spam)\n",
    "\n",
    "decision_tree = DecisionTree(training_data_titanic, training_labels_titanic, range(len(training_labels_titanic)), None)\n",
    "decision_tree.fit(3, True)\n",
    "training_titanic = np.mean(decision_tree.predict(training_data_titanic) == training_labels_titanic)\n",
    "validation_titanic = np.mean(decision_tree.predict(validation_data_titanic) == validation_labels_titanic)\n",
    "\n",
    "forest_training_titanic = random_forest_predict(training_data_titanic, 20, 4, training_data_titanic, training_labels_titanic)\n",
    "forest_validation_titanic = random_forest_predict(validation_data_titanic, 20, 4, training_data_titanic, training_labels_titanic)\n",
    "training_titanic_forest = np.mean(forest_training_titanic == training_labels_titanic)\n",
    "validation_titanic_forest = np.mean(forest_validation_titanic == validation_labels_titanic)\n",
    "\n",
    "print('training for spam Decision Tree: ', training_spam)\n",
    "print('validation for spam Decision Tree: ', validation_spam)\n",
    "print('training for spam Forest: ', training_spam_forest)\n",
    "print('validation for spam Forest: ', validation_spam_forest)\n",
    "\n",
    "print('training for titanic Decision Tree: ', training_titanic)\n",
    "print('validation for titanic Decision Tree: ', validation_titanic)\n",
    "print('training for titanic Forest: ', training_titanic_forest)\n",
    "print('validation for titanic Forest: ', validation_titanic_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Takes a LONG TIME to run\n",
    "tree_training_errors = []\n",
    "tree_validation_errors = []\n",
    "\n",
    "depths = range(1,41)\n",
    "\n",
    "for depth in depths:\n",
    "    test_tree = DecisionTree(training_data_spam, training_labels_spam, range(len(training_labels_spam)), None)\n",
    "    test_tree.fit(depth, True)\n",
    "\n",
    "    tree_validation_errors.append(np.mean(test_tree.predict(validation_data_spam) == validation_labels_spam))\n",
    "    tree_training_errors.append(np.mean(test_tree.predict(training_data_spam) == training_labels_spam))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
